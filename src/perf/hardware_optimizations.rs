//! ğŸš€ ç¡¬ä»¶çº§æ€§èƒ½ä¼˜åŒ– - CPUç¼“å­˜è¡Œå¯¹é½ & SIMDåŠ é€Ÿ
//! 
//! å®ç°CPUç¡¬ä»¶ç‰¹æ€§çš„æ·±åº¦åˆ©ç”¨ï¼ŒåŒ…æ‹¬ï¼š
//! - ç¼“å­˜è¡Œå¯¹é½å’Œç¼“å­˜é¢„å–
//! - SIMDæŒ‡ä»¤é›†ä¼˜åŒ–
//! - åˆ†æ”¯é¢„æµ‹ä¼˜åŒ–
//! - å†…å­˜å±éšœæ§åˆ¶
//! - CPUæŒ‡ä»¤æµæ°´çº¿ä¼˜åŒ–

use std::sync::atomic::{AtomicU64, Ordering};
use std::mem::size_of;
use std::ptr;
use crossbeam_utils::CachePadded;
use anyhow::Result;

// CPUç¼“å­˜è¡Œå¤§å°å¸¸é‡ (é€šå¸¸ä¸º64å­—èŠ‚)
pub const CACHE_LINE_SIZE: usize = 64;

/// ğŸš€ ç¡¬ä»¶ä¼˜åŒ–çš„æ•°æ®ç»“æ„åŸºç¡€ç‰¹å¾
pub trait CacheLineAligned {
    /// ç¡®ä¿æ•°æ®ç»“æ„æŒ‰ç¼“å­˜è¡Œå¯¹é½
    fn ensure_cache_aligned(&self) -> bool;
    /// é¢„å–æ•°æ®åˆ°CPUç¼“å­˜
    fn prefetch_data(&self);
}

/// ğŸš€ SIMDä¼˜åŒ–çš„å†…å­˜æ“ä½œ
pub struct SIMDMemoryOps;

impl SIMDMemoryOps {
    /// ğŸš€ SIMDåŠ é€Ÿçš„å†…å­˜æ‹·è´ - é’ˆå¯¹å°æ•°æ®åŒ…ä¼˜åŒ–
    #[inline(always)]
    pub unsafe fn memcpy_simd_optimized(dst: *mut u8, src: *const u8, len: usize) {
        match len {
            // é’ˆå¯¹ä¸åŒæ•°æ®å¤§å°ä½¿ç”¨ä¸åŒä¼˜åŒ–ç­–ç•¥
            0 => return,
            1..=8 => Self::memcpy_small(dst, src, len),
            9..=16 => Self::memcpy_sse(dst, src, len),
            17..=32 => Self::memcpy_avx(dst, src, len),
            33..=64 => Self::memcpy_avx2(dst, src, len),
            _ => Self::memcpy_avx512_or_fallback(dst, src, len),
        }
    }
    
    /// å°æ•°æ®æ‹·è´ä¼˜åŒ– (1-8å­—èŠ‚)
    #[inline(always)]
    unsafe fn memcpy_small(dst: *mut u8, src: *const u8, len: usize) {
        match len {
            1 => *dst = *src,
            2 => *(dst as *mut u16) = *(src as *const u16),
            3 => {
                *(dst as *mut u16) = *(src as *const u16);
                *dst.add(2) = *src.add(2);
            }
            4 => *(dst as *mut u32) = *(src as *const u32),
            5..=8 => {
                *(dst as *mut u64) = *(src as *const u64);
                if len > 8 {
                    ptr::copy_nonoverlapping(src.add(8), dst.add(8), len - 8);
                }
            }
            _ => unreachable!(),
        }
    }
    
    /// SSEä¼˜åŒ–æ‹·è´ (9-16å­—èŠ‚)
    #[inline(always)]
    unsafe fn memcpy_sse(dst: *mut u8, src: *const u8, len: usize) {
        #[cfg(target_arch = "x86_64")]
        {
            use std::arch::x86_64::{__m128i, _mm_loadu_si128, _mm_storeu_si128};
            
            if len <= 16 {
                let chunk = _mm_loadu_si128(src as *const __m128i);
                _mm_storeu_si128(dst as *mut __m128i, chunk);
            }
        }
        
        #[cfg(not(target_arch = "x86_64"))]
        {
            ptr::copy_nonoverlapping(src, dst, len);
        }
    }
    
    /// AVXä¼˜åŒ–æ‹·è´ (17-32å­—èŠ‚)
    #[inline(always)]
    unsafe fn memcpy_avx(dst: *mut u8, src: *const u8, len: usize) {
        #[cfg(target_arch = "x86_64")]
        {
            use std::arch::x86_64::{__m256i, _mm256_loadu_si256, _mm256_storeu_si256};
            
            if len <= 32 {
                let chunk = _mm256_loadu_si256(src as *const __m256i);
                _mm256_storeu_si256(dst as *mut __m256i, chunk);
            }
        }
        
        #[cfg(not(target_arch = "x86_64"))]
        {
            ptr::copy_nonoverlapping(src, dst, len);
        }
    }
    
    /// AVX2ä¼˜åŒ–æ‹·è´ (33-64å­—èŠ‚)
    #[inline(always)]
    unsafe fn memcpy_avx2(dst: *mut u8, src: *const u8, len: usize) {
        #[cfg(target_arch = "x86_64")]
        {
            use std::arch::x86_64::{__m256i, _mm256_loadu_si256, _mm256_storeu_si256};
            
            // æ‹·è´å‰32å­—èŠ‚
            let chunk1 = _mm256_loadu_si256(src as *const __m256i);
            _mm256_storeu_si256(dst as *mut __m256i, chunk1);
            
            if len > 32 {
                // æ‹·è´å‰©ä½™å­—èŠ‚
                let remaining = len - 32;
                if remaining <= 32 {
                    let chunk2 = _mm256_loadu_si256(src.add(32) as *const __m256i);
                    _mm256_storeu_si256(dst.add(32) as *mut __m256i, chunk2);
                }
            }
        }
        
        #[cfg(not(target_arch = "x86_64"))]
        {
            ptr::copy_nonoverlapping(src, dst, len);
        }
    }
    
    /// AVX512æˆ–å›é€€æ‹·è´ (>64å­—èŠ‚)
    #[inline(always)]
    unsafe fn memcpy_avx512_or_fallback(dst: *mut u8, src: *const u8, len: usize) {
        #[cfg(all(target_arch = "x86_64", target_feature = "avx512f"))]
        {
            use std::arch::x86_64::{__m512i, _mm512_loadu_si512, _mm512_storeu_si512};
            
            let chunks = len / 64;
            let mut offset = 0;
            
            // ä½¿ç”¨AVX512å¤„ç†64å­—èŠ‚å—
            for _ in 0..chunks {
                let chunk = _mm512_loadu_si512(src.add(offset) as *const __m512i);
                _mm512_storeu_si512(dst.add(offset) as *mut __m512i, chunk);
                offset += 64;
            }
            
            // å¤„ç†å‰©ä½™å­—èŠ‚
            let remaining = len % 64;
            if remaining > 0 {
                Self::memcpy_avx2(dst.add(offset), src.add(offset), remaining);
            }
        }
        
        #[cfg(not(all(target_arch = "x86_64", target_feature = "avx512f")))]
        {
            // å›é€€åˆ°AVX2åˆ†å—å¤„ç†
            let chunks = len / 32;
            let mut offset = 0;
            
            for _ in 0..chunks {
                Self::memcpy_avx2(dst.add(offset), src.add(offset), 32);
                offset += 32;
            }
            
            let remaining = len % 32;
            if remaining > 0 {
                Self::memcpy_avx(dst.add(offset), src.add(offset), remaining);
            }
        }
    }
    
    /// ğŸš€ SIMDåŠ é€Ÿçš„å†…å­˜æ¯”è¾ƒ
    #[inline(always)]
    pub unsafe fn memcmp_simd_optimized(a: *const u8, b: *const u8, len: usize) -> bool {
        match len {
            0 => true,
            1..=8 => Self::memcmp_small(a, b, len),
            9..=16 => Self::memcmp_sse(a, b, len),
            17..=32 => Self::memcmp_avx2(a, b, len),
            _ => Self::memcmp_large(a, b, len),
        }
    }
    
    /// å°æ•°æ®æ¯”è¾ƒ
    #[inline(always)]
    unsafe fn memcmp_small(a: *const u8, b: *const u8, len: usize) -> bool {
        match len {
            1 => *a == *b,
            2 => {
                // ä½¿ç”¨ unaligned read é¿å…å¯¹é½é—®é¢˜
                let a_val = (a as *const u16).read_unaligned();
                let b_val = (b as *const u16).read_unaligned();
                a_val == b_val
            }
            3 => {
                let a_val = (a as *const u16).read_unaligned();
                let b_val = (b as *const u16).read_unaligned();
                a_val == b_val && *a.add(2) == *b.add(2)
            }
            4 => {
                let a_val = (a as *const u32).read_unaligned();
                let b_val = (b as *const u32).read_unaligned();
                a_val == b_val
            }
            5..=8 => {
                let a_val = (a as *const u64).read_unaligned();
                let b_val = (b as *const u64).read_unaligned();
                a_val == b_val
            }
            _ => unreachable!(),
        }
    }
    
    /// SSEæ¯”è¾ƒ
    #[inline(always)]
    unsafe fn memcmp_sse(a: *const u8, b: *const u8, len: usize) -> bool {
        #[cfg(target_arch = "x86_64")]
        {
            use std::arch::x86_64::{__m128i, _mm_loadu_si128, _mm_cmpeq_epi8, _mm_movemask_epi8};
            
            let chunk_a = _mm_loadu_si128(a as *const __m128i);
            let chunk_b = _mm_loadu_si128(b as *const __m128i);
            let cmp_result = _mm_cmpeq_epi8(chunk_a, chunk_b);
            let mask = _mm_movemask_epi8(cmp_result) as u32;
            
            // æ£€æŸ¥å‰lenå­—èŠ‚æ˜¯å¦ç›¸ç­‰
            let valid_mask = if len >= 16 { 0xFFFF } else { (1u32 << len) - 1 };
            (mask & valid_mask) == valid_mask
        }
        
        #[cfg(not(target_arch = "x86_64"))]
        {
            (0..len).all(|i| *a.add(i) == *b.add(i))
        }
    }
    
    /// AVX2æ¯”è¾ƒ
    #[inline(always)]
    unsafe fn memcmp_avx2(a: *const u8, b: *const u8, len: usize) -> bool {
        #[cfg(target_arch = "x86_64")]
        {
            use std::arch::x86_64::{__m256i, _mm256_loadu_si256, _mm256_cmpeq_epi8, _mm256_movemask_epi8};
            
            let chunk_a = _mm256_loadu_si256(a as *const __m256i);
            let chunk_b = _mm256_loadu_si256(b as *const __m256i);
            let cmp_result = _mm256_cmpeq_epi8(chunk_a, chunk_b);
            let mask = _mm256_movemask_epi8(cmp_result) as u32;
            
            let valid_mask = if len >= 32 { 0xFFFFFFFF } else { (1u32 << len) - 1 };
            (mask & valid_mask) == valid_mask
        }
        
        #[cfg(not(target_arch = "x86_64"))]
        {
            (0..len).all(|i| *a.add(i) == *b.add(i))
        }
    }
    
    /// å¤§æ•°æ®æ¯”è¾ƒ
    #[inline(always)]
    unsafe fn memcmp_large(a: *const u8, b: *const u8, len: usize) -> bool {
        let chunks = len / 32;
        
        for i in 0..chunks {
            let offset = i * 32;
            if !Self::memcmp_avx2(a.add(offset), b.add(offset), 32) {
                return false;
            }
        }
        
        let remaining = len % 32;
        if remaining > 0 {
            return Self::memcmp_avx2(a.add(chunks * 32), b.add(chunks * 32), remaining);
        }
        
        true
    }
    
    /// ğŸš€ SIMDåŠ é€Ÿçš„å†…å­˜æ¸…é›¶
    #[inline(always)]
    pub unsafe fn memzero_simd_optimized(ptr: *mut u8, len: usize) {
        #[cfg(target_arch = "x86_64")]
        {
            use std::arch::x86_64::{__m256i, _mm256_setzero_si256, _mm256_storeu_si256};
            
            let zero = _mm256_setzero_si256();
            let chunks = len / 32;
            let mut offset = 0;
            
            for _ in 0..chunks {
                _mm256_storeu_si256(ptr.add(offset) as *mut __m256i, zero);
                offset += 32;
            }
            
            // å¤„ç†å‰©ä½™å­—èŠ‚
            let remaining = len % 32;
            for i in 0..remaining {
                *ptr.add(offset + i) = 0;
            }
        }
        
        #[cfg(not(target_arch = "x86_64"))]
        {
            ptr::write_bytes(ptr, 0, len);
        }
    }
}

/// ğŸš€ ç¼“å­˜è¡Œå¯¹é½çš„åŸå­è®¡æ•°å™¨
#[repr(align(64))] // å¼ºåˆ¶64å­—èŠ‚å¯¹é½
pub struct CacheAlignedCounter {
    value: AtomicU64,
    _padding: [u8; CACHE_LINE_SIZE - size_of::<AtomicU64>()],
}

impl CacheAlignedCounter {
    pub fn new(initial: u64) -> Self {
        Self {
            value: AtomicU64::new(initial),
            _padding: [0; CACHE_LINE_SIZE - size_of::<AtomicU64>()],
        }
    }
    
    #[inline(always)]
    pub fn increment(&self) -> u64 {
        self.value.fetch_add(1, Ordering::Relaxed)
    }
    
    #[inline(always)]
    pub fn load(&self) -> u64 {
        self.value.load(Ordering::Relaxed)
    }
    
    #[inline(always)]
    pub fn store(&self, val: u64) {
        self.value.store(val, Ordering::Relaxed)
    }
}

impl CacheLineAligned for CacheAlignedCounter {
    fn ensure_cache_aligned(&self) -> bool {
        (self as *const Self as usize) % CACHE_LINE_SIZE == 0
    }
    
    fn prefetch_data(&self) {
        #[cfg(target_arch = "x86_64")]
        unsafe {
            use std::arch::x86_64::_mm_prefetch;
            use std::arch::x86_64::_MM_HINT_T0;
            _mm_prefetch(self as *const Self as *const i8, _MM_HINT_T0);
        }
    }
}

/// ğŸš€ ç¼“å­˜å‹å¥½çš„ç¯å½¢ç¼“å†²åŒº
#[repr(align(64))]
pub struct CacheOptimizedRingBuffer<T> {
    /// æ•°æ®ç¼“å†²åŒº
    buffer: Vec<T>,
    /// ç”Ÿäº§è€…å¤´æŒ‡é’ˆ (ç‹¬å ç¼“å­˜è¡Œ)
    producer_head: CachePadded<AtomicU64>,
    /// æ¶ˆè´¹è€…å°¾æŒ‡é’ˆ (ç‹¬å ç¼“å­˜è¡Œ) 
    consumer_tail: CachePadded<AtomicU64>,
    /// å®¹é‡ (2çš„å¹‚æ¬¡æ–¹)
    capacity: usize,
    /// æ©ç  (capacity - 1)
    mask: usize,
}

impl<T: Copy + Default> CacheOptimizedRingBuffer<T> {
    /// åˆ›å»ºç¼“å­˜ä¼˜åŒ–çš„ç¯å½¢ç¼“å†²åŒº
    pub fn new(capacity: usize) -> Result<Self> {
        if !capacity.is_power_of_two() {
            return Err(anyhow::anyhow!("Capacity must be a power of 2"));
        }
        
        let mut buffer = Vec::with_capacity(capacity);
        buffer.resize_with(capacity, Default::default);
        
        Ok(Self {
            buffer,
            producer_head: CachePadded::new(AtomicU64::new(0)),
            consumer_tail: CachePadded::new(AtomicU64::new(0)),
            capacity,
            mask: capacity - 1,
        })
    }
    
    /// ğŸš€ æ— é”å†™å…¥å…ƒç´ 
    #[inline(always)]
    pub fn try_push(&self, item: T) -> bool {
        let current_head = self.producer_head.load(Ordering::Relaxed);
        let current_tail = self.consumer_tail.load(Ordering::Acquire);
        
        // æ£€æŸ¥æ˜¯å¦è¿˜æœ‰ç©ºé—´
        if (current_head + 1) & self.mask as u64 == current_tail & self.mask as u64 {
            return false; // ç¼“å†²åŒºæ»¡
        }
        
        // å†™å…¥æ•°æ®
        unsafe {
            let index = current_head & self.mask as u64;
            let ptr = self.buffer.as_ptr().add(index as usize) as *mut T;
            ptr.write(item);
        }
        
        // å‘å¸ƒæ–°çš„å¤´æŒ‡é’ˆ
        self.producer_head.store(current_head + 1, Ordering::Release);
        true
    }
    
    /// ğŸš€ æ— é”è¯»å–å…ƒç´ 
    #[inline(always)]
    pub fn try_pop(&self) -> Option<T> {
        let current_tail = self.consumer_tail.load(Ordering::Relaxed);
        let current_head = self.producer_head.load(Ordering::Acquire);
        
        // æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®
        if current_tail == current_head {
            return None; // ç¼“å†²åŒºç©º
        }
        
        // è¯»å–æ•°æ®
        let item = unsafe {
            let index = current_tail & self.mask as u64;
            let ptr = self.buffer.as_ptr().add(index as usize);
            ptr.read()
        };
        
        // å‘å¸ƒæ–°çš„å°¾æŒ‡é’ˆ
        self.consumer_tail.store(current_tail + 1, Ordering::Release);
        Some(item)
    }
    
    /// è·å–å½“å‰å…ƒç´ æ•°é‡
    #[inline(always)]
    pub fn len(&self) -> usize {
        let head = self.producer_head.load(Ordering::Relaxed);
        let tail = self.consumer_tail.load(Ordering::Relaxed);
        ((head + self.capacity as u64 - tail) & self.mask as u64) as usize
    }
    
    /// æ£€æŸ¥æ˜¯å¦ä¸ºç©º
    #[inline(always)]
    pub fn is_empty(&self) -> bool {
        self.producer_head.load(Ordering::Relaxed) == 
        self.consumer_tail.load(Ordering::Relaxed)
    }
}

impl<T> CacheLineAligned for CacheOptimizedRingBuffer<T> {
    fn ensure_cache_aligned(&self) -> bool {
        (self as *const Self as usize) % CACHE_LINE_SIZE == 0
    }
    
    fn prefetch_data(&self) {
        #[cfg(target_arch = "x86_64")]
        unsafe {
            use std::arch::x86_64::_mm_prefetch;
            use std::arch::x86_64::_MM_HINT_T0;
            
            // é¢„å–å¤´æŒ‡é’ˆ
            _mm_prefetch(self.producer_head.as_ptr() as *const i8, _MM_HINT_T0);
            
            // é¢„å–å°¾æŒ‡é’ˆ
            _mm_prefetch(self.consumer_tail.as_ptr() as *const i8, _MM_HINT_T0);
            
            // é¢„å–ç¼“å†²åŒºå¼€å§‹ä½ç½®
            _mm_prefetch(self.buffer.as_ptr() as *const i8, _MM_HINT_T0);
        }
    }
}

/// ğŸš€ CPUåˆ†æ”¯é¢„æµ‹ä¼˜åŒ–å·¥å…·
pub struct BranchOptimizer;

impl BranchOptimizer {
    /// likelyå® - å‘Šè¯‰ç¼–è¯‘å™¨æ¡ä»¶å¤§æ¦‚ç‡ä¸ºçœŸ
    #[inline(always)]
    pub fn likely(condition: bool) -> bool {
        #[cold]
        fn cold() {}
        
        if !condition {
            cold();
        }
        condition
    }
    
    /// unlikelyå® - å‘Šè¯‰ç¼–è¯‘å™¨æ¡ä»¶å¤§æ¦‚ç‡ä¸ºå‡
    #[inline(always)]
    pub fn unlikely(condition: bool) -> bool {
        #[cold]
        fn cold() {}
        
        if condition {
            cold();
        }
        condition
    }
    
    /// é¢„å–æŒ‡ä»¤ - æå‰åŠ è½½æ•°æ®åˆ°ç¼“å­˜
    #[inline(always)]
    pub unsafe fn prefetch_read_data<T>(_ptr: *const T) {
        #[cfg(target_arch = "x86_64")]
        {
            use std::arch::x86_64::_mm_prefetch;
            use std::arch::x86_64::_MM_HINT_T0;
            _mm_prefetch(_ptr as *const i8, _MM_HINT_T0);
        }
    }
    
    /// é¢„å–æŒ‡ä»¤ - æå‰åŠ è½½æ•°æ®åˆ°ç¼“å­˜ï¼ˆå†™ä¼˜åŒ–ï¼‰
    #[inline(always)]
    pub unsafe fn prefetch_write_data<T>(_ptr: *const T) {
        #[cfg(target_arch = "x86_64")]
        {
            use std::arch::x86_64::_mm_prefetch;
            use std::arch::x86_64::_MM_HINT_T1;
            _mm_prefetch(_ptr as *const i8, _MM_HINT_T1);
        }
    }
}

/// ğŸš€ å†…å­˜å±éšœæ§åˆ¶
pub struct MemoryBarriers;

impl MemoryBarriers {
    /// ç¼–è¯‘å™¨å±éšœ - é˜²æ­¢ç¼–è¯‘å™¨é‡æ’åº
    #[inline(always)]
    pub fn compiler_barrier() {
        std::sync::atomic::compiler_fence(Ordering::SeqCst);
    }
    
    /// è½»é‡çº§å†…å­˜å±éšœ - ä»…CPUé‡æ’åºä¿æŠ¤
    #[inline(always)]
    pub fn memory_barrier_light() {
        std::sync::atomic::fence(Ordering::Acquire);
    }
    
    /// é‡é‡çº§å†…å­˜å±éšœ - å…¨åºä¸€è‡´æ€§
    #[inline(always)]
    pub fn memory_barrier_heavy() {
        std::sync::atomic::fence(Ordering::SeqCst);
    }
    
    /// å­˜å‚¨å±éšœ - ç¡®ä¿å†™å…¥å¯è§æ€§
    #[inline(always)]
    pub fn store_barrier() {
        std::sync::atomic::fence(Ordering::Release);
    }
    
    /// åŠ è½½å±éšœ - ç¡®ä¿è¯»å–æ­£ç¡®æ€§
    #[inline(always)]
    pub fn load_barrier() {
        std::sync::atomic::fence(Ordering::Acquire);
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    
    #[test]
    fn test_cache_aligned_counter() {
        let counter = CacheAlignedCounter::new(0);
        assert!(counter.ensure_cache_aligned());
        
        assert_eq!(counter.load(), 0);
        counter.increment();
        assert_eq!(counter.load(), 1);
    }
    
    #[test]
    fn test_simd_memcpy() {
        let src = [1u8, 2, 3, 4, 5, 6, 7, 8, 9, 10];
        let mut dst = [0u8; 10];
        
        unsafe {
            SIMDMemoryOps::memcpy_simd_optimized(
                dst.as_mut_ptr(), 
                src.as_ptr(), 
                src.len()
            );
        }
        
        assert_eq!(src, dst);
    }
    
    #[test]
    fn test_cache_optimized_ring_buffer() {
        let buffer: CacheOptimizedRingBuffer<u64> = 
            CacheOptimizedRingBuffer::new(16).unwrap();
        
        assert!(buffer.is_empty());
        
        // æµ‹è¯•æ¨å…¥
        assert!(buffer.try_push(42));
        assert_eq!(buffer.len(), 1);
        
        // æµ‹è¯•å¼¹å‡º
        assert_eq!(buffer.try_pop(), Some(42));
        assert!(buffer.is_empty());
    }
    
    #[test]
    fn test_simd_memcmp() {
        let a = [1u8, 2, 3, 4, 5];
        let b = [1u8, 2, 3, 4, 5];
        let c = [1u8, 2, 3, 4, 6];
        
        unsafe {
            assert!(SIMDMemoryOps::memcmp_simd_optimized(
                a.as_ptr(), b.as_ptr(), a.len()
            ));
            
            assert!(!SIMDMemoryOps::memcmp_simd_optimized(
                a.as_ptr(), c.as_ptr(), a.len()
            ));
        }
    }
}